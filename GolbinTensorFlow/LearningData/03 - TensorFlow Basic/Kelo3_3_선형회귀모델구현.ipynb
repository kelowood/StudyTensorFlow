{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 회귀는 간단하게 말하자면 주어진 x, y 값을 가지고 서로간의 관계를 파악하는 것이다.\n",
    "# (책 44페이지 참조) 2차 선형 회귀는 2차 그래프의 직선을 파악하여 입력에 대한 출력을 산출해 낼 수 있는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_8:0' shape=(1,) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_9:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# x와 y 데이터를 정의한다.\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "# x, y의 상관관계를 설명하기 위한 변수 W, b를 정의한다.\n",
    "# 이 두 값은 스칼라 값이며, -1.0 ~ 1.0 범위의 균등 분포를 가진 무작위값으로 초기화된다.\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "print(W)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"X_3:0\", dtype=float32)\n",
      "Tensor(\"Y_3:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 다음은 자료를 입력받기 위해 필요한 플레이스 홀더이다.\n",
    "# 자료형은 float32의 스칼라 값이다.\n",
    "X = tf.placeholder(tf.float32, name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "print(X)\n",
    "print(Y)\n",
    "\n",
    "# 여기서는 플레이스 홀더 정의시 name 매개변수에 값을 넣음으로써 이름을 설정하는 것을 알 수 있다.\n",
    "# 위와 같이 이름을 넣어줄 시 어떤 텐서가 어떻게 사용되고 있는지 쉽게 알 수 있다. \n",
    "# 그리고 차후에 사용할 디버깅도구인 텐서보드에서도 이 이름을 통해 디버깅을 수월하게 할 수 있다.\n",
    "# 이름은 플레이스 홀더 뿐만 아니라 변수, 연산 및 연산함수에도 지정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_1:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# X, Y의 상관관계를 분석하기 위한 수식을 작성한다.\n",
    "hypothesis = W * X + b\n",
    "print(hypothesis)\n",
    "\n",
    "# 이 수식은 W와의 곱과 b와의 합을 통하여 X, Y의 관계를 설명하겠다는 뜻이다.\n",
    "# 즉, 주어진 X, Y가 있을때 이것에 적합한 W, b 값을 찾아내겠다는 의미이기도 하다.\n",
    "# 여기서 곱연산을 하는 W는 가중치(Weight)라 하고, b는 편향(bias)라고 부른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 다음은 손실함수에 대한 작성이다.\n",
    "# 손실함수(loss function)는 데이터에 대한 손실값을 나타내는 함수이다.\n",
    "# 손실값 : X에 대한 실제값(Y)과 모델로 예측한 값이 얼마나 차이가 나는지를 나타내는 값\n",
    "# 손실값이 작을수록 X에 대한 Y값을 정확히 예측할 수 있게 된다.\n",
    "# 이 손실값을 전체 데이터에 대해 구한 경우 이것을 비용(cost)이라고 부른다.\n",
    "# 즉 학습 이라 함은. 다양한 값들을 넣어봄으로써 손실값을 최소화하는 W, b 값을 구하는 것을 말한다.\n",
    "\n",
    "# 손실값은 '예측값과 실제값의 거리'를 가장 많이 쓴다.\n",
    "# 이 값은 예측 값에서 실제 값을 뺀 다음 제곱하여 산출한다.\n",
    "# 그리고 비용 값은 모든 데이터들에 대한 손실값의 평균을 내어서 구한다.\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "print(cost)\n",
    "\n",
    "# tf.square()는 각 요소의 제곱을 구하는 함수이다.\n",
    "# tf.reduce_mean(x)는 x의 각 요소에 대한 평균 값을 구하는 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.training.gradient_descent.GradientDescentOptimizer object at 0x000001A99D719828>\n",
      "name: \"GradientDescent_1\"\n",
      "op: \"NoOp\"\n",
      "input: \"^GradientDescent_1/update_Variable_8/ApplyGradientDescent\"\n",
      "input: \"^GradientDescent_1/update_Variable_9/ApplyGradientDescent\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 최적화 함수란 가중치와 편향 값을 변경해 가면서 손실값을 최소화하는 최적의 두 값을 찾아주는 함수이다.\n",
    "# 이 두 값을 구할때 값들을 단순히 무작위로 변경한다면 시간이 너무 오래 걸리고 학습시간도 예측하기가 어렵다.\n",
    "# 그렇기 때문에 빠르게 최적화하기 위한 다양한 방법을 사용하게 된다.\n",
    "# 아래 사용하게 될 경사하강법(Gradient descent) 최적화 함수는 가장 기본적인 알고리즘으로 \n",
    "# 함수의 기울기가 낮은쪽으로 계속 이동시키면서 최족의 값을 찾아나가는 방법이다.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train_op = optimizer.minimize(cost)\n",
    "print(optimizer)\n",
    "print(train_op)\n",
    "\n",
    "# 여기서 쓰인 학습률(learning_rate)은 학습을 얼마나 급하게 할것인지 설정하는 값이다.\n",
    "# 이 값이 너무 크면 최적의 손실값을 찾지 못하고 지나쳐 버린다.\n",
    "# 이 값이 너무 작으면 학습 속도가 느려진다.\n",
    "# 이와 같이 학습을 진행하는 과정에 영향을 미치는 변수를 하이퍼파라미터(hyperparameter)라고 부른다.\n",
    "# 런닝머신은 이 하이퍼 파라미터를 어떻게 튜닝하냐에 따라 성능이 크게 틀려 질수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.05871815 [0.8050599] [0.42272824]\n",
      "1 0.026413426 [0.8179127] [0.41615862]\n",
      "2 0.024806172 [0.82139736] [0.4057618]\n",
      "3 0.02362367 [0.82578844] [0.3960505]\n",
      "4 0.022501493 [0.8299657] [0.38652503]\n",
      "5 0.021432618 [0.83405435] [0.37723374]\n",
      "6 0.020414585 [0.83804345] [0.36816525]\n",
      "7 0.01944487 [0.84193677] [0.35931483]\n",
      "8 0.018521225 [0.8457365] [0.35067716]\n",
      "9 0.017641446 [0.84944487] [0.34224713]\n",
      "10 0.016803479 [0.8530642] [0.33401978]\n",
      "11 0.016005298 [0.85659635] [0.32599014]\n",
      "12 0.015245032 [0.8600437] [0.3181536]\n",
      "13 0.01452088 [0.86340815] [0.3105054]\n",
      "14 0.013831117 [0.8666917] [0.30304107]\n",
      "15 0.013174143 [0.86989635] [0.29575616]\n",
      "16 0.012548369 [0.873024] [0.2886464]\n",
      "17 0.01195231 [0.87607634] [0.28170753]\n",
      "18 0.011384555 [0.8790554] [0.27493548]\n",
      "19 0.010843795 [0.88196284] [0.26832625]\n",
      "20 0.010328691 [0.8848004] [0.26187587]\n",
      "21 0.009838087 [0.8875697] [0.25558054]\n",
      "22 0.009370771 [0.89027244] [0.24943654]\n",
      "23 0.008925637 [0.8929102] [0.24344026]\n",
      "24 0.008501675 [0.8954846] [0.23758814]\n",
      "25 0.00809783 [0.8979971] [0.23187667]\n",
      "26 0.0077131875 [0.90044916] [0.2263025]\n",
      "27 0.007346796 [0.9028423] [0.22086234]\n",
      "28 0.0069978223 [0.9051779] [0.21555296]\n",
      "29 0.0066654123 [0.9074573] [0.2103712]\n",
      "30 0.006348805 [0.90968204] [0.20531404]\n",
      "31 0.0060472395 [0.9118532] [0.20037842]\n",
      "32 0.0057599894 [0.9139722] [0.19556147]\n",
      "33 0.0054863817 [0.91604024] [0.19086029]\n",
      "34 0.0052257758 [0.9180585] [0.18627211]\n",
      "35 0.0049775415 [0.9200283] [0.18179427]\n",
      "36 0.0047411076 [0.9219509] [0.17742409]\n",
      "37 0.0045158984 [0.92382705] [0.17315891]\n",
      "38 0.0043013967 [0.9256583] [0.16899632]\n",
      "39 0.0040970673 [0.92744535] [0.16493374]\n",
      "40 0.0039024649 [0.92918956] [0.16096886]\n",
      "41 0.0037170977 [0.9308918] [0.15709928]\n",
      "42 0.003540527 [0.93255305] [0.15332268]\n",
      "43 0.003372348 [0.9341745] [0.14963692]\n",
      "44 0.0032121574 [0.93575686] [0.14603975]\n",
      "45 0.0030595837 [0.9373012] [0.14252906]\n",
      "46 0.0029142473 [0.93880844] [0.13910276]\n",
      "47 0.0027758155 [0.9402795] [0.13575885]\n",
      "48 0.0026439654 [0.94171506] [0.13249528]\n",
      "49 0.0025183726 [0.9431162] [0.12931019]\n",
      "50 0.0023987482 [0.9444837] [0.12620169]\n",
      "51 0.0022848116 [0.94581825] [0.12316788]\n",
      "52 0.0021762757 [0.94712067] [0.12020699]\n",
      "53 0.0020729003 [0.9483919] [0.11731732]\n",
      "54 0.0019744404 [0.9496326] [0.1144971]\n",
      "55 0.0018806513 [0.95084333] [0.11174465]\n",
      "56 0.0017913185 [0.95202506] [0.10905839]\n",
      "57 0.0017062267 [0.9531783] [0.10643668]\n",
      "58 0.0016251864 [0.95430386] [0.10387803]\n",
      "59 0.0015479826 [0.9554024] [0.10138087]\n",
      "60 0.0014744537 [0.9564745] [0.09894376]\n",
      "61 0.0014044206 [0.9575208] [0.09656521]\n",
      "62 0.001337703 [0.958542] [0.09424385]\n",
      "63 0.001274164 [0.9595386] [0.09197829]\n",
      "64 0.0012136388 [0.9605112] [0.08976719]\n",
      "65 0.0011559926 [0.96146053] [0.08760926]\n",
      "66 0.0011010804 [0.962387] [0.08550321]\n",
      "67 0.0010487809 [0.9632912] [0.08344777]\n",
      "68 0.0009989614 [0.9641736] [0.08144172]\n",
      "69 0.0009515101 [0.9650349] [0.07948393]\n",
      "70 0.00090631214 [0.96587545] [0.0775732]\n",
      "71 0.0008632606 [0.9666957] [0.07570837]\n",
      "72 0.00082225545 [0.9674964] [0.07388841]\n",
      "73 0.00078319997 [0.96827775] [0.07211219]\n",
      "74 0.00074599584 [0.96904033] [0.07037865]\n",
      "75 0.00071056193 [0.9697846] [0.06868681]\n",
      "76 0.0006768119 [0.97051096] [0.06703562]\n",
      "77 0.0006446605 [0.9712198] [0.0654241]\n",
      "78 0.0006140366 [0.97191167] [0.06385136]\n",
      "79 0.0005848705 [0.9725869] [0.06231642]\n",
      "80 0.0005570858 [0.9732459] [0.06081837]\n",
      "81 0.0005306246 [0.97388905] [0.05935632]\n",
      "82 0.0005054222 [0.97451675] [0.05792945]\n",
      "83 0.00048141318 [0.97512937] [0.05653686]\n",
      "84 0.00045854645 [0.9757272] [0.05517774]\n",
      "85 0.00043676244 [0.97631073] [0.05385131]\n",
      "86 0.0004160185 [0.9768802] [0.05255675]\n",
      "87 0.00039625526 [0.97743595] [0.05129331]\n",
      "88 0.00037743387 [0.9779784] [0.05006027]\n",
      "89 0.0003595079 [0.97850776] [0.04885685]\n",
      "90 0.00034242982 [0.97902447] [0.04768239]\n",
      "91 0.0003261631 [0.97952867] [0.04653613]\n",
      "92 0.00031066927 [0.98002076] [0.04541743]\n",
      "93 0.0002959119 [0.98050106] [0.04432564]\n",
      "94 0.00028185805 [0.98096985] [0.0432601]\n",
      "95 0.0002684709 [0.98142725] [0.04222014]\n",
      "96 0.00025571723 [0.98187375] [0.0412052]\n",
      "97 0.00024356955 [0.9823095] [0.04021467]\n",
      "98 0.00023200041 [0.98273474] [0.03924792]\n",
      "99 0.00022097891 [0.98314977] [0.03830443]\n",
      "X: 5, Y: [4.954053]\n",
      "X: 2.5, Y: [2.4961786]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # 전 장과 마찬가지로 세션 생성 후 변수들을 초기화한다.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 아래 반복문을 통하여 최적화를 수행하는 train_op 값을 구한다.\n",
    "    # 이 작업을 하면서 변경되는 W, b, 비용(cost)값 또한 같이 뽑아내어 출력해보도록 한다.\n",
    "    for step in range(100):\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})  \n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "        \n",
    "    # train_op를 반복적으로 실행시켜 적절한 W, b를 구해내었다면 이제 X값을 넣어보고 적절한 Y값 결과를 확인해 보도록 하자.\n",
    "    print(\"X: 5, Y:\", sess.run(hypothesis, feed_dict={X: 5}))\n",
    "    print(\"X: 2.5, Y:\", sess.run(hypothesis, feed_dict={X: 2.5}))\n",
    "    \n",
    "    # 결과를 보다시피 예상된 값과 굉장히 근접한 값이 나옴을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
