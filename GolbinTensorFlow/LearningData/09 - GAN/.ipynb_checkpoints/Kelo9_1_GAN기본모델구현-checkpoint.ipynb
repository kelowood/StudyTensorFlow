{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN (Generative Adversarial Network)\n",
    "# 오토 인코더와 마찬가지로 결과물을 생성하는 생성 모델 중 하나이다.\n",
    "# 서로 대립(adversarial)하는 두 신경망을 경쟁시켜가면서 결과물 생성방법을 학습하는 모델\n",
    "\n",
    "# 구분자와 생성자를 잘 이해해야 한다.\n",
    "# 위조지폐범과 정상지폐를 구분하는 경찰과의 관계를 참고하자 (책. 148페이지 참조)\n",
    "\n",
    "# 구분자 (Discriminator)\n",
    "# 경찰 역활. 이미지를 주고 이것이 진짜인지 가짜인지 판별하게 하는 존재.\n",
    "# 가장 먼저는 실제 이미지를 준다음 이것이 진짜임을 판단하게 해야 한다.\n",
    "# 그 다음 생성자를 통해 노이즈로부터 만들어진 임의의 이미지를 만들고 \n",
    "# 이것을 해당 구분자를 통해 가짜임을 판별하도록 한다.\n",
    "# 생성자의 이미지가 진짜인지 가짜인지 잘 분류할 수 있도록 학습하는 것이 주 목표\n",
    "\n",
    "# 생성자 (Generator)\n",
    "# 위조지폐범 역활. 구분자가 진짜라고 판단할 정도로 이미지를 만들어내도록 훈련\n",
    "# 구분자가 구분을 못하도록 흡사하게 이미지를 만들어내도록 학습하는 것이 주 목표\n",
    "\n",
    "# 구분자와 생성자의 경쟁을 통해 \n",
    "# 결과적으로 생성자는 실제 이미지와 상당히 비슷한 결과를 만들어내게 된다.\n",
    "\n",
    "# 이 코드는 MNIST 손글씨 숫자를 무작위로 생성하는 간단한 예제를 만들어본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# tensorflow, matplotlib.pyplot, numpy, minst 튜토리얼 데이터를 임포트한다.\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 설정하기\n",
    "hp_total_epoch = 100\n",
    "hp_batch_size = 100\n",
    "hp_learning_rate = 0.0002\n",
    "hp_n_hidden = 256\n",
    "hp_n_input = 28*28\n",
    "hp_n_noise = 128  # 생성자의 입력값으로 사용할 노이즈의 크기\n",
    "\n",
    "# 플레이스 홀더 설정\n",
    "# GAN도 비지도 학습이므로 Y값이 존재하지 않는다.\n",
    "# 하지만 구분자에 넣을 이미지가 실제 이미지와 생성된 가짜이미지 두개이며,\n",
    "# 가짜 이미지는 노이즈에서 생성할 예정이므로 노이즈를 입력할 플레이스홀더 Z를 추가한다.\n",
    "X = tf.placeholder(tf.float32, [None, hp_n_input])\n",
    "Z = tf.placeholder(tf.float32, [None, hp_n_noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 설정\n",
    "\n",
    "# 첫번째로 생성자(Generator) 신경망에 사용할 변수를 설정한다.\n",
    "# 노이즈입력 -> 은닉층 -> 출력층으로 가는 형태이다.\n",
    "# 생성자의 출력층 갯수는 구분자에서의 당연히 원본 이미지 픽셀수와 같아야 한다.\n",
    "VGen_W1 = tf.Variable(tf.random_normal([hp_n_noise, hp_n_hidden], stddev=0.01))\n",
    "VGen_b1 = tf.Variable(tf.zeros([hp_n_hidden]))\n",
    "VGen_W2 = tf.Variable(tf.random_normal([hp_n_hidden, hp_n_input]))\n",
    "VGen_b2 = tf.Variable(tf.zeros([hp_n_input]))\n",
    "\n",
    "# 그 다음으로 구분자(Discriminator) 신경망에 사용할 변수를 설정한다.\n",
    "# 은닉층 갯수는 생성자 때와 동일하다. \n",
    "# 구분자는 출력층 갯수로 딱 하나로써 진짜와 얼마나 가까운가를 판단하는 값이 된다.\n",
    "VDis_W1 = tf.Variable(tf.random_normal([hp_n_input, hp_n_hidden], stddev=0.01))\n",
    "VDis_b1 = tf.Variable(tf.zeros([hp_n_hidden]))\n",
    "VDis_W2 = tf.Variable(tf.random_normal([hp_n_hidden, 1], stddev=0.01))\n",
    "VDis_b2 = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 생성자와 구분자 신경망을 구성해보자\n",
    "\n",
    "# 생성자 신경망 함수이다.\n",
    "# 활성화 함수는 sigmoid를 적용하였다.\n",
    "def Generator(noise_z):\n",
    "    hidden = tf.nn.relu(\n",
    "        tf.matmul(noise_z, VGen_W1) + VGen_b1)\n",
    "    output = tf.nn.sigmoid(\n",
    "        tf.matmul(hidden, VGen_W2) + VGen_b2)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# 구분자 신경망 함수이다.\n",
    "# 역시나 활성화 함수는 sigmoid가 들어갔다.\n",
    "def Discriminator(inputs):\n",
    "    hidden = tf.nn.relu(\n",
    "        tf.matmul(inputs, VDis_W1) + VDis_b1)\n",
    "    output = tf.nn.sigmoid(\n",
    "        tf.matmul(hidden, VDis_W2) + VDis_b2)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번엔 무작위 노이즈를 만들어주는 간단한 유틸리티 함수를 만들자\n",
    "def GenerateNoise(_batchSize, _numNoise):\n",
    "    return np.random.normal(size=(_batchSize, _numNoise))\n",
    "\n",
    "# 그 다음 생성자를 호출시켜 가짜 이미지를 만들고, \n",
    "# 가짜 이미지와 원본 이미지를 구분자에 넣어 진위여부를 판단하도록 한다.\n",
    "G = Generator(Z)\n",
    "D_gene = Discriminator(G)\n",
    "D_real = Discriminator(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제는 손실값을 구해야 하는데 이번에는 두개가 필요하다.\n",
    "\n",
    "# 1) 생성자가 만든 이미지를 구분자가 가짜라고 판단하도록 하는 손실값 (경찰 학습용)\n",
    "# 2) 생성자가 만든 이미지를 진짜라고 판단하도록 하는 손실값 (위조지폐범 학습용)\n",
    "\n",
    "# 1)의 경우는 D_real이 1에 가까워져야 하고, D_gene가 0에 가까워져야 한다.\n",
    "loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1 - D_gene))\n",
    "# 위의 한줄을 간단히 설명하자면 (D_real)과 (1 - D_gene)을 각각 로그처리하여 더한 값을 손실값으로 한다는 것이다.\n",
    "\n",
    "# 2)의 경우는 가짜 이미지 판별값 D_gene을 1에 가깝게 만들기만 하면 된다.\n",
    "loss_G = tf.reduce_mean(tf.log(D_gene))\n",
    "\n",
    "# 결국 GAN에서의 학습은 loss_D와 loss_G를 최대화 시키는 것이다.\n",
    "# 하지만, loss_D와 loss_G는 서로 연관되어 있으므로 두 손실값이 항상 같이 증가하지는 않을 것이다.\n",
    "# loss_D가 증가하면 loss_G는 감소할 것이고, 반대로 loss_G가 증가하면 loss_D는 감소하게 될 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 손실값들을 이용하여 학습시키는 일이 남았다\n",
    "# 근데 여기서 주의점이 있다.\n",
    "# loss_D를 구할때는 구분자 신경망에서 사용되는 변수들만 사용하고, \n",
    "# loss_G를 구할때는 생성자 신경망에서 사용되는 변수들만 사용해서 최적화해야 한다.\n",
    "# 그렇게 해야 loss_D를 학습할때 생성자가 변하지 않고,\n",
    "# loss_G를 학습할때는 구분자가 변하지 않기 때문이다.\n",
    "D_var_list = [VDis_W1, VDis_b1, VDis_W2, VDis_b2]\n",
    "G_var_list = [VGen_W1, VGen_b1, VGen_W2, VGen_b2]\n",
    "\n",
    "# 이제 최적화 함수를 정의해보자.\n",
    "# GAN 논문에 따르면 loss를 최대화 해야 하는 것이지만 \n",
    "# 최적화에 쓸수 있는 함수는 minimize()밖에 없으므로.\n",
    "# 최적화 하려는 loss_D와 loss_G에 음수를 붙여주도록 한다.\n",
    "train_D = tf.train.AdamOptimizer(hp_learning_rate).minimize(-loss_D, var_list=D_var_list)\n",
    "train_G = tf.train.AdamOptimizer(hp_learning_rate).minimize(-loss_G, var_list=G_var_list)\n",
    "\n",
    "# minimize()내에서 var_list 매개변수의 인자를 주게 되면 해당 \n",
    "# 변수 리스트내 요소들만 학습하게 된다는 것을 알수있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 D loss: -0.4866 G loss: -3.344\n",
      "Epoch: 0001 D loss: -0.363 G loss: -2.756\n",
      "Epoch: 0002 D loss: -0.5576 G loss: -1.646\n",
      "Epoch: 0003 D loss: -0.4067 G loss: -1.8\n",
      "Epoch: 0004 D loss: -0.5145 G loss: -1.959\n",
      "Epoch: 0005 D loss: -0.5386 G loss: -2.018\n",
      "Epoch: 0006 D loss: -0.6436 G loss: -1.749\n",
      "Epoch: 0007 D loss: -0.5928 G loss: -1.657\n",
      "Epoch: 0008 D loss: -0.5665 G loss: -1.88\n",
      "Epoch: 0009 D loss: -0.5838 G loss: -1.896\n",
      "Epoch: 0010 D loss: -0.5951 G loss: -1.792\n",
      "Epoch: 0011 D loss: -0.504 G loss: -1.972\n",
      "Epoch: 0012 D loss: -0.5098 G loss: -2.024\n",
      "Epoch: 0013 D loss: -0.5422 G loss: -2.013\n",
      "Epoch: 0014 D loss: -0.4866 G loss: -1.868\n",
      "Epoch: 0015 D loss: -0.5499 G loss: -1.885\n",
      "Epoch: 0016 D loss: -0.591 G loss: -1.897\n",
      "Epoch: 0017 D loss: -0.5835 G loss: -2.24\n",
      "Epoch: 0018 D loss: -0.5383 G loss: -2.18\n",
      "Epoch: 0019 D loss: -0.5836 G loss: -2.103\n",
      "Epoch: 0020 D loss: -0.4632 G loss: -2.387\n",
      "Epoch: 0021 D loss: -0.4485 G loss: -2.373\n",
      "Epoch: 0022 D loss: -0.4411 G loss: -2.294\n",
      "Epoch: 0023 D loss: -0.5176 G loss: -2.336\n",
      "Epoch: 0024 D loss: -0.3673 G loss: -2.424\n",
      "Epoch: 0025 D loss: -0.3904 G loss: -2.782\n",
      "Epoch: 0026 D loss: -0.406 G loss: -2.491\n",
      "Epoch: 0027 D loss: -0.5064 G loss: -2.423\n",
      "Epoch: 0028 D loss: -0.292 G loss: -2.783\n",
      "Epoch: 0029 D loss: -0.4698 G loss: -2.899\n",
      "Epoch: 0030 D loss: -0.3865 G loss: -2.788\n",
      "Epoch: 0031 D loss: -0.3394 G loss: -2.785\n",
      "Epoch: 0032 D loss: -0.4134 G loss: -2.898\n",
      "Epoch: 0033 D loss: -0.31 G loss: -2.786\n",
      "Epoch: 0034 D loss: -0.2718 G loss: -3.14\n",
      "Epoch: 0035 D loss: -0.3173 G loss: -3.304\n",
      "Epoch: 0036 D loss: -0.2526 G loss: -3.244\n",
      "Epoch: 0037 D loss: -0.2531 G loss: -2.817\n",
      "Epoch: 0038 D loss: -0.3338 G loss: -2.969\n",
      "Epoch: 0039 D loss: -0.3024 G loss: -3.019\n",
      "Epoch: 0040 D loss: -0.2577 G loss: -3.35\n",
      "Epoch: 0041 D loss: -0.234 G loss: -3.05\n",
      "Epoch: 0042 D loss: -0.2899 G loss: -3.29\n",
      "Epoch: 0043 D loss: -0.4096 G loss: -3.329\n",
      "Epoch: 0044 D loss: -0.2575 G loss: -3.208\n",
      "Epoch: 0045 D loss: -0.3009 G loss: -2.954\n",
      "Epoch: 0046 D loss: -0.3086 G loss: -2.978\n",
      "Epoch: 0047 D loss: -0.2152 G loss: -3.159\n",
      "Epoch: 0048 D loss: -0.2342 G loss: -3.345\n",
      "Epoch: 0049 D loss: -0.1985 G loss: -3.347\n",
      "Epoch: 0050 D loss: -0.1771 G loss: -3.208\n",
      "Epoch: 0051 D loss: -0.2087 G loss: -3.637\n",
      "Epoch: 0052 D loss: -0.1729 G loss: -3.454\n",
      "Epoch: 0053 D loss: -0.2082 G loss: -3.126\n",
      "Epoch: 0054 D loss: -0.1654 G loss: -3.589\n",
      "Epoch: 0055 D loss: -0.2031 G loss: -3.107\n",
      "Epoch: 0056 D loss: -0.1637 G loss: -3.425\n",
      "Epoch: 0057 D loss: -0.2032 G loss: -3.371\n",
      "Epoch: 0058 D loss: -0.1573 G loss: -3.865\n",
      "Epoch: 0059 D loss: -0.1899 G loss: -3.695\n",
      "Epoch: 0060 D loss: -0.1296 G loss: -4.072\n",
      "Epoch: 0061 D loss: -0.2076 G loss: -3.583\n",
      "Epoch: 0062 D loss: -0.1446 G loss: -3.936\n",
      "Epoch: 0063 D loss: -0.1873 G loss: -3.724\n",
      "Epoch: 0064 D loss: -0.1604 G loss: -3.813\n",
      "Epoch: 0065 D loss: -0.136 G loss: -3.745\n",
      "Epoch: 0066 D loss: -0.2003 G loss: -4.604\n",
      "Epoch: 0067 D loss: -0.1004 G loss: -3.864\n",
      "Epoch: 0068 D loss: -0.1149 G loss: -4.091\n",
      "Epoch: 0069 D loss: -0.159 G loss: -4.076\n",
      "Epoch: 0070 D loss: -0.176 G loss: -4.298\n",
      "Epoch: 0071 D loss: -0.171 G loss: -3.444\n",
      "Epoch: 0072 D loss: -0.1486 G loss: -3.868\n",
      "Epoch: 0073 D loss: -0.1739 G loss: -4.093\n",
      "Epoch: 0074 D loss: -0.117 G loss: -4.462\n",
      "Epoch: 0075 D loss: -0.1446 G loss: -4.172\n",
      "Epoch: 0076 D loss: -0.129 G loss: -4.258\n",
      "Epoch: 0077 D loss: -0.1394 G loss: -3.929\n",
      "Epoch: 0078 D loss: -0.1246 G loss: -3.633\n",
      "Epoch: 0079 D loss: -0.1598 G loss: -4.106\n",
      "Epoch: 0080 D loss: -0.1316 G loss: -4.313\n",
      "Epoch: 0081 D loss: -0.07211 G loss: -4.474\n",
      "Epoch: 0082 D loss: -0.166 G loss: -3.787\n",
      "Epoch: 0083 D loss: -0.1455 G loss: -3.899\n",
      "Epoch: 0084 D loss: -0.08631 G loss: -3.676\n",
      "Epoch: 0085 D loss: -0.147 G loss: -5.048\n",
      "Epoch: 0086 D loss: -0.1034 G loss: -4.301\n",
      "Epoch: 0087 D loss: -0.1043 G loss: -4.443\n",
      "Epoch: 0088 D loss: -0.1284 G loss: -4.676\n",
      "Epoch: 0089 D loss: -0.08455 G loss: -4.197\n",
      "Epoch: 0090 D loss: -0.08327 G loss: -4.868\n",
      "Epoch: 0091 D loss: -0.09277 G loss: -4.488\n",
      "Epoch: 0092 D loss: -0.1547 G loss: -4.367\n",
      "Epoch: 0093 D loss: -0.1281 G loss: -4.191\n",
      "Epoch: 0094 D loss: -0.1669 G loss: -4.232\n",
      "Epoch: 0095 D loss: -0.1562 G loss: -4.526\n",
      "Epoch: 0096 D loss: -0.04343 G loss: -5.014\n",
      "Epoch: 0097 D loss: -0.04109 G loss: -4.538\n",
      "Epoch: 0098 D loss: -0.1221 G loss: -4.148\n",
      "Epoch: 0099 D loss: -0.07964 G loss: -4.478\n",
      "최적화 완료.\n"
     ]
    }
   ],
   "source": [
    "# 이제 그래프는 모두 구성했고 세션을 만들어서 실행해보자.\n",
    "# 이번에는 두개의 손실값을 학습해야 하므로 코드가 좀 틀릴 수 있다.\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "num_total_batch = int(mnist.train.num_examples / hp_batch_size)\n",
    "res_loss_val_D, res_loss_val_G = 0, 0\n",
    "\n",
    "# 미니배치로 학습을 반복한다.\n",
    "for epoch in range(hp_total_epoch):\n",
    "    for i in range(num_total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(hp_batch_size)\n",
    "        noise = GenerateNoise(hp_batch_size, hp_n_noise)\n",
    "        \n",
    "        _, res_loss_val_D = sess.run(\n",
    "            [train_D, loss_D],\n",
    "            feed_dict={X: batch_xs, Z: noise})\n",
    "        _, res_loss_val_G = sess.run(\n",
    "            [train_G, loss_G],\n",
    "            feed_dict={Z: noise})\n",
    "        \n",
    "    print(\n",
    "        'Epoch:', '%04d' % epoch,\n",
    "        'D loss: {:.4}'.format(res_loss_val_D),\n",
    "        'G loss: {:.4}'.format(res_loss_val_G))\n",
    "        \n",
    "    # 확인용 이미지를 만들어보도록 하자.\n",
    "    # 0, 9. 19, 29, ...번째 에포크마다 생성기로 이미지를 생성하여 눈으로 직접 확인해보도록 하자.\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        sample_size = 10\n",
    "        noise = GenerateNoise(sample_size, hp_n_noise)\n",
    "        samples = sess.run(G, feed_dict={Z: noise})\n",
    "        \n",
    "        fig, ax = plt.subplots(2, sample_size, figsize=(sample_size, 2))\n",
    "        \n",
    "        for i in range(sample_size):\n",
    "            ax[0][i].set_axis_off()\n",
    "            ax[1][i].set_axis_off()\n",
    "            \n",
    "            ax[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28)))\n",
    "            ax[1][i].imshow(np.reshape(samples[i], (28, 28)))\n",
    "            \n",
    "        plt.savefig(\n",
    "            'samples/{}.png'.format(str(epoch).zfill(3)),\n",
    "            bbox_inches='tight')\n",
    "        \n",
    "        plt.close(fig)\n",
    "\n",
    "print('최적화 완료.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
