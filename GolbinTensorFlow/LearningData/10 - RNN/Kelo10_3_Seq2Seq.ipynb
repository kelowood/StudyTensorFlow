{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence to Sequence <Seq2Seq>\n",
    "# RNN 신경망과 출력하는 신경망을 조합한 모델\n",
    "# 번역이나 챗봇등에서 많이 사용된다.\n",
    "# 구글 기계번역에서도 이것을 사용하고 있다.\n",
    "\n",
    "# Seq2Seq 모델은 크게 두가지로 구성된다.\n",
    "# 1) 인코더 : 입력을 위한 신경망. 원문을 입력으로 받음.\n",
    "# 2) 디코더 : 출력을 위한 신경망. 인코더가 번역한 결과물을 입력으로 받음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq 심볼은 특수한 심볼이 몇개 필요하다. \n",
    "# (골빈해커책의 195 페이지 참조)\n",
    "# S 심볼 : 디코더에 입력이 시작됨을 알려주는 심볼\n",
    "# E 심볼 : 디코더의 출력이 끝났음을 알려주는 심볼\n",
    "# P 심볼 : 빈 데이터를 채울때 사용하는 무의미 심볼\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 이제 데이터를 만들어보자. 먼저 영어알파벳과 한글들을 나열한 뒤에 한글자씩 배열에 집어넣어보자.\n",
    "\n",
    "\n",
    "char_arr = [charcter for charcter in 'SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀키스사랑']\n",
    "num_dic = {n: i for i , n in enumerate(char_arr)}\n",
    "dic_len = len(num_dic)\n",
    "\n",
    "seq_data = [\n",
    "    ['word', '단어'],\n",
    "    ['wood', '나무'],\n",
    "    ['game', '놀이'],\n",
    "    ['girl', '소녀'],\n",
    "    ['kiss', '키스'],\n",
    "    ['love', '사랑']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그다음 입력 단어와 출력 단어를 한글자씩 떼어내서 배열로 만든 다음,\n",
    "# 원-핫 인코딩 형태로 바꿔주는 함수를 만든다.\n",
    "# 리턴 데이터는 아래와 같이 3가지로 구성된다.\n",
    "# 1) 인코더의 입력값\n",
    "# 2) 디코더의 입력값\n",
    "# 3) 디코더의 출력값\n",
    "\n",
    "def MakeBatch(_seqData):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for iterSeq in _seqData:\n",
    "        \n",
    "        print('MakeBatch() Process<iterSeq>:', iterSeq, ' ------------')\n",
    "        \n",
    "        # 1) 인코더 입력값 : 입력단어('word')를 한글자씩 떼어서 인덱스 배열로 만든다.\n",
    "        inputData = [num_dic[n] for n in iterSeq[0]]\n",
    "        # 2) 디코더 입력값 : 출력단어('단어') 앞에 디코더 시작을 나타내는 S 글자를 붙인다.\n",
    "        # 그 후 한글자씩 떼어서 인덱스 배열로 만든다.\n",
    "        outputData = [num_dic[n] for n in ('S' + iterSeq[1])]\n",
    "        # 3) 디코더 출력값 : 출력단어('단어') 마지막에 디코더 끝을 알리는 심볼 'E' 글자를 붙인다.\n",
    "        # 그 후 한글자씩 떼어서 인덱스 배열로 만든다.\n",
    "        targetData = [num_dic[n] for n in (iterSeq[1] + 'E')]\n",
    "        \n",
    "        print('MakeBatch() inputData:', inputData)\n",
    "        print('MakeBatch() outputData:', outputData)\n",
    "        print('MakeBatch() targetData:', targetData)\n",
    "        print('----------------------------------------------------')\n",
    "        \n",
    "        # 인코더 입력값, 디코더 입력값은 원-핫 인코딩 형태로 인코딩하여 리스트에 추가하고,\n",
    "        # 디코더 출력값만 정수 인덱스 값 그대로 리스트에 추가한다.\n",
    "        input_batch.append(np.eye(dic_len)[inputData])\n",
    "        output_batch.append(np.eye(dic_len)[outputData])\n",
    "        target_batch.append(targetData)\n",
    "    \n",
    "    return input_batch, output_batch, target_batch       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터의 정의\n",
    "\n",
    "hp_learning_rate = 0.01\n",
    "hp_n_hidden = 128\n",
    "hp_total_epoch = 100\n",
    "\n",
    "hp_n_class = hp_n_input = dic_len\n",
    "\n",
    "# 이제 플레이스홀더를 만들어보고자 한다.\n",
    "# 여기에서 인코더 및 디코더의 입력값 형식은 아래와 같다.\n",
    "# [batchSize, timeSteps, inputSize]\n",
    "# 여기에서 inputSize는 hp_n_input 즉 딕셔너리의 길이, char_arr로 정의한 글자들의 갯수를 뜻한다.\n",
    "\n",
    "# 그리고 디코더 출력값의 형식은 아래와 같다.\n",
    "# [batchSize, timeSteps]\n",
    "\n",
    "enc_input = tf.placeholder(tf.float32, [None, None, hp_n_input])\n",
    "dec_input = tf.placeholder(tf.float32, [None, None, hp_n_input])\n",
    "targets = tf.placeholder(tf.int64, [None, None])\n",
    "\n",
    "# 잘 이해가 가지 않는 부분이다.\n",
    "# 여기에서 timeSteps는 글자수를 의미하는 것이라고 한다. 즉, 'word'면 4글자이고, '단어'라면 두글자가 될 것이다.\n",
    "# 다음 부분에서 알아봐야 할 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RNN 모델을 위한 셀 구성 부분이다.\n",
    "# 인코더 셀과 디코더 셀 이렇게 두 종류의 셀이 구성된다.\n",
    "\n",
    "with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicRNNCell(hp_n_hidden)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "    \n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input, dtype=tf.float32)\n",
    "\n",
    "# 여기서 중요한점이 디코더를 만들때 초기 상태값으로 인코더의 최종 상태값을 넣어주어야 한다는 것이다.\n",
    "# seq2seq의 핵심중 하나는 인코더에서 계산된 상태를 디코더로 전파하는 것이다.\n",
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(hp_n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "    \n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input, initial_state=enc_states, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음으로 출력층을 만들고 손실함수, 최적화 함수를 구성해보자\n",
    "model = tf.layers.dense(outputs, hp_n_class, activation=None)\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=model, labels=targets))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(hp_learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MakeBatch() Process<iterSeq>: ['word', '단어']  ------------\n",
      "MakeBatch() inputData: [25, 17, 20, 6]\n",
      "MakeBatch() outputData: [0, 29, 30]\n",
      "MakeBatch() targetData: [29, 30, 1]\n",
      "----------------------------------------------------\n",
      "MakeBatch() Process<iterSeq>: ['wood', '나무']  ------------\n",
      "MakeBatch() inputData: [25, 17, 17, 6]\n",
      "MakeBatch() outputData: [0, 31, 32]\n",
      "MakeBatch() targetData: [31, 32, 1]\n",
      "----------------------------------------------------\n",
      "MakeBatch() Process<iterSeq>: ['game', '놀이']  ------------\n",
      "MakeBatch() inputData: [9, 3, 15, 7]\n",
      "MakeBatch() outputData: [0, 33, 34]\n",
      "MakeBatch() targetData: [33, 34, 1]\n",
      "----------------------------------------------------\n",
      "MakeBatch() Process<iterSeq>: ['girl', '소녀']  ------------\n",
      "MakeBatch() inputData: [9, 11, 20, 14]\n",
      "MakeBatch() outputData: [0, 35, 36]\n",
      "MakeBatch() targetData: [35, 36, 1]\n",
      "----------------------------------------------------\n",
      "MakeBatch() Process<iterSeq>: ['kiss', '키스']  ------------\n",
      "MakeBatch() inputData: [13, 11, 21, 21]\n",
      "MakeBatch() outputData: [0, 37, 38]\n",
      "MakeBatch() targetData: [37, 38, 1]\n",
      "----------------------------------------------------\n",
      "MakeBatch() Process<iterSeq>: ['love', '사랑']  ------------\n",
      "MakeBatch() inputData: [14, 17, 24, 7]\n",
      "MakeBatch() outputData: [0, 39, 40]\n",
      "MakeBatch() targetData: [39, 40, 1]\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 세션을 시작하기전에 여기서 먼저 seq_data로부터 나오는 배치 데이터들이 어떤형태로 나오는지 봐보자.\n",
    "input_batch, output_batch, target_batch = MakeBatch(seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eopch: 0001 cost =  3.673878\n",
      "Eopch: 0002 cost =  2.492944\n",
      "Eopch: 0003 cost =  1.442095\n",
      "Eopch: 0004 cost =  1.300584\n",
      "Eopch: 0005 cost =  0.739203\n",
      "Eopch: 0006 cost =  0.514042\n",
      "Eopch: 0007 cost =  0.508415\n",
      "Eopch: 0008 cost =  0.219122\n",
      "Eopch: 0009 cost =  0.159701\n",
      "Eopch: 0010 cost =  0.199794\n",
      "Eopch: 0011 cost =  0.134862\n",
      "Eopch: 0012 cost =  0.087829\n",
      "Eopch: 0013 cost =  0.190977\n",
      "Eopch: 0014 cost =  0.219430\n",
      "Eopch: 0015 cost =  0.095242\n",
      "Eopch: 0016 cost =  0.037138\n",
      "Eopch: 0017 cost =  0.075893\n",
      "Eopch: 0018 cost =  0.022738\n",
      "Eopch: 0019 cost =  0.024171\n",
      "Eopch: 0020 cost =  0.015450\n",
      "Eopch: 0021 cost =  0.015676\n",
      "Eopch: 0022 cost =  0.020635\n",
      "Eopch: 0023 cost =  0.022981\n",
      "Eopch: 0024 cost =  0.022153\n",
      "Eopch: 0025 cost =  0.013171\n",
      "Eopch: 0026 cost =  0.009018\n",
      "Eopch: 0027 cost =  0.012243\n",
      "Eopch: 0028 cost =  0.009991\n",
      "Eopch: 0029 cost =  0.004602\n",
      "Eopch: 0030 cost =  0.015245\n",
      "Eopch: 0031 cost =  0.004732\n",
      "Eopch: 0032 cost =  0.001530\n",
      "Eopch: 0033 cost =  0.003412\n",
      "Eopch: 0034 cost =  0.006275\n",
      "Eopch: 0035 cost =  0.003181\n",
      "Eopch: 0036 cost =  0.001577\n",
      "Eopch: 0037 cost =  0.002710\n",
      "Eopch: 0038 cost =  0.002779\n",
      "Eopch: 0039 cost =  0.001753\n",
      "Eopch: 0040 cost =  0.002213\n",
      "Eopch: 0041 cost =  0.003122\n",
      "Eopch: 0042 cost =  0.001784\n",
      "Eopch: 0043 cost =  0.000954\n",
      "Eopch: 0044 cost =  0.004090\n",
      "Eopch: 0045 cost =  0.001641\n",
      "Eopch: 0046 cost =  0.001052\n",
      "Eopch: 0047 cost =  0.001645\n",
      "Eopch: 0048 cost =  0.002347\n",
      "Eopch: 0049 cost =  0.001731\n",
      "Eopch: 0050 cost =  0.000591\n",
      "Eopch: 0051 cost =  0.000629\n",
      "Eopch: 0052 cost =  0.003991\n",
      "Eopch: 0053 cost =  0.000492\n",
      "Eopch: 0054 cost =  0.004099\n",
      "Eopch: 0055 cost =  0.001145\n",
      "Eopch: 0056 cost =  0.000710\n",
      "Eopch: 0057 cost =  0.000871\n",
      "Eopch: 0058 cost =  0.003428\n",
      "Eopch: 0059 cost =  0.000878\n",
      "Eopch: 0060 cost =  0.001452\n",
      "Eopch: 0061 cost =  0.000385\n",
      "Eopch: 0062 cost =  0.000634\n",
      "Eopch: 0063 cost =  0.000360\n",
      "Eopch: 0064 cost =  0.001512\n",
      "Eopch: 0065 cost =  0.000323\n",
      "Eopch: 0066 cost =  0.001050\n",
      "Eopch: 0067 cost =  0.000378\n",
      "Eopch: 0068 cost =  0.003261\n",
      "Eopch: 0069 cost =  0.000777\n",
      "Eopch: 0070 cost =  0.000556\n",
      "Eopch: 0071 cost =  0.000490\n",
      "Eopch: 0072 cost =  0.000500\n",
      "Eopch: 0073 cost =  0.001478\n",
      "Eopch: 0074 cost =  0.000370\n",
      "Eopch: 0075 cost =  0.000417\n",
      "Eopch: 0076 cost =  0.000629\n",
      "Eopch: 0077 cost =  0.000414\n",
      "Eopch: 0078 cost =  0.002564\n",
      "Eopch: 0079 cost =  0.001404\n",
      "Eopch: 0080 cost =  0.000132\n",
      "Eopch: 0081 cost =  0.000431\n",
      "Eopch: 0082 cost =  0.000427\n",
      "Eopch: 0083 cost =  0.000465\n",
      "Eopch: 0084 cost =  0.000290\n",
      "Eopch: 0085 cost =  0.000764\n",
      "Eopch: 0086 cost =  0.000627\n",
      "Eopch: 0087 cost =  0.000850\n",
      "Eopch: 0088 cost =  0.000637\n",
      "Eopch: 0089 cost =  0.000934\n",
      "Eopch: 0090 cost =  0.000412\n",
      "Eopch: 0091 cost =  0.004313\n",
      "Eopch: 0092 cost =  0.000309\n",
      "Eopch: 0093 cost =  0.000501\n",
      "Eopch: 0094 cost =  0.000141\n",
      "Eopch: 0095 cost =  0.000912\n",
      "Eopch: 0096 cost =  0.001111\n",
      "Eopch: 0097 cost =  0.001098\n",
      "Eopch: 0098 cost =  0.000514\n",
      "Eopch: 0099 cost =  0.002122\n",
      "Eopch: 0100 cost =  0.000180\n",
      "최적화 완료\n"
     ]
    }
   ],
   "source": [
    "# 세션을 실행해보자\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(hp_total_epoch):\n",
    "    _, loss = sess.run(\n",
    "        [optimizer, cost],\n",
    "        feed_dict=\n",
    "        {\n",
    "            enc_input: input_batch,\n",
    "            dec_input: output_batch,\n",
    "            targets: target_batch\n",
    "        })\n",
    "    \n",
    "    print('Eopch:', '%04d' % (epoch + 1),\n",
    "          'cost = ', '{:.6f}'.format(loss))\n",
    "    \n",
    "print('최적화 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 확인하기 위해서 단어를 입력받아 번역 단어를 예측하는 함수를 만들어보자\n",
    "def Translate(_word):\n",
    "    seqData = [_word, 'P' * len(_word)]\n",
    "    \n",
    "    input_batch, output_batch, target_batch = MakeBatch([seqData])\n",
    "    \n",
    "    # 여기에서 _word 인자로 'kiss'를 받았다면 seqData는 ['kiss', 'PPPP']로 구성될 것이다.\n",
    "    # 사전에 설명했다시피 'P' 글자는 빈 데이터를 채울때 사용하는 무의미 심볼을 의미한다.\n",
    "    # input_batch는 ['k', 'i', 's', 's'], output_batch는 ['P', 'P', 'P', 'P'] 글자들의 인덱스를 원-핫인코딩한 것들이다.\n",
    "    # target_batch는 ['P', 'P', 'P', 'P']\n",
    "    \n",
    "    # 예측 모델을 돌려보자\n",
    "    # print('model:', model)\n",
    "    # model의 출력 형태는 [batchSize, timeSteps, inputSize] 형태인데 여기서 세번째 차원 inputSize의 argmax값을 사용한다.\n",
    "    prediction = tf.argmax(model, 2)\n",
    "\n",
    "    result = sess.run(\n",
    "        prediction,\n",
    "        feed_dict=\n",
    "        {\n",
    "            enc_input: input_batch,\n",
    "            dec_input: output_batch,\n",
    "            targets: target_batch\n",
    "        })\n",
    "    \n",
    "    # 예측 결과는 글자의 인덱스를 뜻하는 숫자이므로 각 숫자에 해당하는 글자를 가져와서 배열을 만든다.\n",
    "    # 그리고 출력의 끝을 의미하는 E 이후의 글자들을 제거하고 문자열로 만든다. \n",
    "    # 출력은 디코더의 입력 크기(time steps) 만큼 출력값이 나오므로 최종결과는 ['사', '랑', 'E', 'E'] 처럼 나오기 때문이다.\n",
    "    decoded = [char_arr[i] for i in result[0]]\n",
    "    \n",
    "    end = decoded.index('E')\n",
    "    translated = ''.join(decoded[:end])\n",
    "    \n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 번역 테스트 ===\n",
      "MakeBatch() Process<iterSeq>: ['word', 'PPPP']  ------------\n",
      "MakeBatch() inputData: [25, 17, 20, 6]\n",
      "MakeBatch() outputData: [0, 2, 2, 2, 2]\n",
      "MakeBatch() targetData: [2, 2, 2, 2, 1]\n",
      "----------------------------------------------------\n",
      "MakeBatch() Process<iterSeq>: ['wodr', 'PPPP']  ------------\n",
      "MakeBatch() inputData: [25, 17, 6, 20]\n",
      "MakeBatch() outputData: [0, 2, 2, 2, 2]\n",
      "MakeBatch() targetData: [2, 2, 2, 2, 1]\n",
      "----------------------------------------------------\n",
      "MakeBatch() Process<iterSeq>: ['love', 'PPPP']  ------------\n",
      "MakeBatch() inputData: [14, 17, 24, 7]\n",
      "MakeBatch() outputData: [0, 2, 2, 2, 2]\n",
      "MakeBatch() targetData: [2, 2, 2, 2, 1]\n",
      "----------------------------------------------------\n",
      "MakeBatch() Process<iterSeq>: ['loev', 'PPPP']  ------------\n",
      "MakeBatch() inputData: [14, 17, 7, 24]\n",
      "MakeBatch() outputData: [0, 2, 2, 2, 2]\n",
      "MakeBatch() targetData: [2, 2, 2, 2, 1]\n",
      "----------------------------------------------------\n",
      "MakeBatch() Process<iterSeq>: ['abcd', 'PPPP']  ------------\n",
      "MakeBatch() inputData: [3, 4, 5, 6]\n",
      "MakeBatch() outputData: [0, 2, 2, 2, 2]\n",
      "MakeBatch() targetData: [2, 2, 2, 2, 1]\n",
      "----------------------------------------------------\n",
      "word -> 단어\n",
      "wodr -> 나무\n",
      "love -> 사랑\n",
      "loev -> 사랑\n",
      "abcd -> 소어\n"
     ]
    }
   ],
   "source": [
    "# 실제 번역테스트를 수행해보도록 하자.\n",
    "print('\\n=== 번역 테스트 ===')\n",
    "\n",
    "trans_word = Translate('word')\n",
    "trans_wodr = Translate('wodr')\n",
    "trans_love = Translate('love')\n",
    "trans_loev = Translate('loev')\n",
    "trans_abcd = Translate('abcd')\n",
    "\n",
    "print('word ->', trans_word)\n",
    "print('wodr ->', trans_wodr)\n",
    "print('love ->', trans_love)\n",
    "print('loev ->', trans_loev)\n",
    "print('abcd ->', trans_abcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
