{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 먼저 DQN에 대한 개념을 알아두어야 한다.\n",
    "# 골빈해커책 224~228의 내용을 잘 확인하고 특히 228 페이지의 그림을 잘 기억해두자.\n",
    "# 현 소스코드인 에이전트는 두가지 역활을 한다.\n",
    "# 1) 게임으로부터 게임의 상태를 기본 신경망으로 전달한다.\n",
    "# 2) 신경망에서 판단한 행동을 게임에 적용한다.\n",
    "# 즉, 이 코드에서 신경망은 구현하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from game import Game\n",
    "from model import DQN\n",
    "# 파이썬 문법\n",
    "# 위의 코드는 아래 두 행동을 하고 있다.\n",
    "# 1) game.py 파일로부터 Game 클래스를 임포트한다.\n",
    "# 2) model.py 파일로부터 DQN 클래스를 임포트한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에이전트의 실행 모드는 1)학습모드 와 2)게임실행모드(replay) 로 나뉜다.\n",
    "# 1) 학습모드 : 게임을 화면에 보여주지 않고 빠르게 실행하여 학습속도를 높인다.\n",
    "# 2) 게임실행모드 : 학습한 결과를 이용하여 게임을 진행하면서 화면에도 출력한다.\n",
    "# 이것을 위해 tf.app.flags를 이용한다.\n",
    "tf.app.flags.DEFINE_boolean(\n",
    "    \"train\",\n",
    "    False,\n",
    "    \"학습모드. 게임을 화면에 보여주지 않습니다.\")\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번에는 하이퍼 파라미터들을 설정한다.\n",
    "HP_MAX_EPISODE = 10000           # 최대로 학습할 게임 횟수\n",
    "HP_TARGET_UPDATE_INTERVAL = 1000 # 학습을 n만큼 진행할때마다 한번씩 \n",
    "HP_TRAIN_INTERVAL = 4            # n번의 게임 프레임 마다 한번씩 학습하라.\n",
    "HP_OBSERVE = 100                 # 게임 프레임이 n 값까지 도달한 이후에 학습을 시작한다.\n",
    "\n",
    "# HP_OBSERVE가 존재하는 이후는 게임 시작 시점의 학습 데이터가 적을때는\n",
    "# 학습을 진행해도 효과가 크기 않기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제는 게임에 필요한 설정을 세팅해보도록 하자.\n",
    "\n",
    "# 행동값 -> \n",
    "# 0 : 좌\n",
    "# 1 : 유지\n",
    "# 2 : 우\n",
    "NUM_ACTION = 3\n",
    "SCREEN_WIDTH = 6\n",
    "SCREEN_HEIGHT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    # 텐서플로 세션과 게임 객체, DQN 모델 객체를 생성한다.\n",
    "    print(\"뇌세포 깨우는 중...\")\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # 게임 내에는 화면 크기와 게임을 보여줄 것인지의 여부(False)를 인자로 준다.\n",
    "    game = Game(SCREEN_WIDTH, SCREEN_HEIGHT, show_game=False)\n",
    "    # DQN 모델 객체에는 텐서플로 세션과 화면크기, 선택할 행동의 갯수를 넣어준다.\n",
    "    brain = DQN(sess, SCREEN_WIDTH, SCREEN_HEIGHT, NUM_ACTION)\n",
    "    \n",
    "    # 이 값은 게임을 한판하고 얻는 점수를 저장하고, 확인하기 위한 텐서이다.\n",
    "    # 차후에는 10판에 한번씩 로그를 저장하고, 그때 rewards의 평균을 저장할 것이다.\n",
    "    rewards = tf.placeholder(tf.float32, [None])\n",
    "    tf.summary.scalar('avg.reward/ep.', tf.reduce_mean(rewards))\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    writer = tf.summary.FileWriter('logs', sess.graph)\n",
    "    summary_merged = tf.summary.merge_all()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
