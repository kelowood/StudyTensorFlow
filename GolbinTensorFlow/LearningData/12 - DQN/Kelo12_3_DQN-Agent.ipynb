{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 먼저 DQN에 대한 개념을 알아두어야 한다.\n",
    "# 골빈해커책 224~228의 내용을 잘 확인하고 특히 228 페이지의 그림을 잘 기억해두자.\n",
    "# 현 소스코드인 에이전트는 두가지 역활을 한다.\n",
    "# 1) 게임으로부터 게임의 상태를 기본 신경망으로 전달한다.\n",
    "# 2) 신경망에서 판단한 행동을 게임에 적용한다.\n",
    "# 즉, 이 코드에서 신경망은 구현하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "from game import Game\n",
    "from model import DQN\n",
    "# 파이썬 문법\n",
    "# 위의 코드는 아래 두 행동을 하고 있다.\n",
    "# 1) game.py 파일로부터 Game 클래스를 임포트한다.\n",
    "# 2) model.py 파일로부터 DQN 클래스를 임포트한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에이전트의 실행 모드는 1)학습모드 와 2)게임실행모드(replay) 로 나뉜다.\n",
    "# 1) 학습모드 : 게임을 화면에 보여주지 않고 빠르게 실행하여 학습속도를 높인다.\n",
    "# 2) 게임실행모드 : 학습한 결과를 이용하여 게임을 진행하면서 화면에도 출력한다.\n",
    "# 이것을 위해 tf.app.flags를 이용한다.\n",
    "tf.app.flags.DEFINE_boolean(\n",
    "    \"train\",\n",
    "    False,\n",
    "    \"학습모드. 게임을 화면에 보여주지 않습니다.\")\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번에는 하이퍼 파라미터들을 설정한다.\n",
    "HP_MAX_EPISODE = 10000           # 최대로 학습할 게임 횟수\n",
    "HP_TARGET_UPDATE_INTERVAL = 1000 # 학습을 n만큼 진행할때마다 한번씩 \n",
    "HP_TRAIN_INTERVAL = 4            # n번의 게임 프레임 마다 한번씩 학습하라.\n",
    "HP_OBSERVE = 100                 # 게임 프레임이 n 값까지 도달한 이후에 학습을 시작한다.\n",
    "\n",
    "# HP_OBSERVE가 존재하는 이후는 게임 시작 시점의 학습 데이터가 적을때는\n",
    "# 학습을 진행해도 효과가 크기 않기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제는 게임에 필요한 설정을 세팅해보도록 하자.\n",
    "\n",
    "# 행동값 -> \n",
    "# 0 : 좌\n",
    "# 1 : 유지\n",
    "# 2 : 우\n",
    "NUM_ACTION = 3\n",
    "SCREEN_WIDTH = 6\n",
    "SCREEN_HEIGHT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    # 텐서플로 세션과 게임 객체, DQN 모델 객체를 생성한다.\n",
    "    print(\"뇌세포 깨우는 중...\")\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # 게임 내에는 화면 크기와 게임을 보여줄 것인지의 여부(False)를 인자로 준다.\n",
    "    game = Game(SCREEN_WIDTH, SCREEN_HEIGHT, show_game=False)\n",
    "    # DQN 모델 객체에는 텐서플로 세션과 화면크기, 선택할 행동의 갯수를 넣어준다.\n",
    "    brain = DQN(sess, SCREEN_WIDTH, SCREEN_HEIGHT, NUM_ACTION)\n",
    "    \n",
    "    # 이 값은 게임을 한판하고 얻는 점수를 저장하고, 확인하기 위한 텐서이다.\n",
    "    # 차후에는 10판에 한번씩 로그를 저장하고, 그때 rewards의 평균을 저장할 것이다.\n",
    "    rewards = tf.placeholder(tf.float32, [None])\n",
    "    tf.summary.scalar('avg.reward/ep.', tf.reduce_mean(rewards))\n",
    "    \n",
    "    # 학습의 결과를 저장하기 위한 객체\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    writer = tf.summary.FileWriter('logs', sess.graph)\n",
    "    summary_merged = tf.summary.merge_all()\n",
    "    \n",
    "    # 모델 신경망을 한번 초기화해준다.\n",
    "    brain.update_target_network()\n",
    "    \n",
    "    # epsilon 값은 행동을 선택할때 DQN을 이용할 시점을 정한다.\n",
    "    # 학습 초반에는 DQN이 항상 같은 값만 내놓을 가능성이 높으므로 초반에는 행동을 무작위로 정한다.\n",
    "    # 그리고 점점 에피소드가 진행할수록 앱실론 값을 줄이므로써 DQN의 학습 데이터를 이용할 수 있도록 한다.\n",
    "    epsilon = 1.0\n",
    "    # 아래 루프에서 계속 1씩 더해진다. <게임 프레임 값>\n",
    "    time_step = 0\n",
    "    total_reward_list = []\n",
    "    \n",
    "    # 최대 에피소드만큼 에피소드를 반복한다.\n",
    "    for episode in range(HP_MAX_EPISODE):\n",
    "        # 터미널 값은 초기에는 False이다.\n",
    "        terminal = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        # 게임 객체의 reset() 을 호출하여 스테이트를 얻는다.\n",
    "        state = game.reset()\n",
    "        # 신경망에서의 스테이트를 초기화한다.\n",
    "        brain.init_state(state)\n",
    "        \n",
    "        # 터미널 값이 True가 될때까지 계속 반복\n",
    "        while not terminal:\n",
    "            # 랜덤 값이 엡실론보다 작다면? \n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.randrange(NUM_ACTION)\n",
    "            else:\n",
    "                action = brain.get_action()\n",
    "            \n",
    "            # 만약 에피소드가 HP_OBSERVE값 보다 커진다면?\n",
    "            # 앱실론 값은 루프를 돌때마다 줄어들게 될 것이다.\n",
    "            if episode > HP_OBSERVE:\n",
    "                epsilon -= 1 / 1000\n",
    "            \n",
    "            # 액션 값을 인자로 게임 스텝을 진행하여 리턴값들을 얻는다.\n",
    "            state, reward, terminal = game.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            brain.remember(state, action, reward, terminal)\n",
    "            \n",
    "            # 타임 스텝 값이 HP_OBSERVE를 넘어가고, 학습 인터벌 값 지점에 도달시\n",
    "            # 신경망에서 트레이닝을 시킨다.\n",
    "            if time_step > HP_OBSERVE and time_step % HP_TRAIN_INTERVAL == 0:\n",
    "                brain.train()\n",
    "                \n",
    "            # 타겟 업데이트 인터벌 값 지점에 도달시\n",
    "            # 신경망에서 타겟 네트워크를 업데이트해준다.\n",
    "            if time_step % HP_TARGET_UPDATE_INTERVAL == 0:\n",
    "                brain.update_target_network()\n",
    "                \n",
    "            time_step += 1\n",
    "        \n",
    "        print('게임횟수: %d 점수: %d' % (episode + 1, total_reward))\n",
    "        \n",
    "        total_reward_list.append(total_reward)\n",
    "        \n",
    "        # 에피소드 10번 중 한번 실행\n",
    "        if episode % 10 == 0:\n",
    "            # summary_merged 텐서를 실행시킨다.\n",
    "            summary = sess.run(\n",
    "                summary_merged,\n",
    "                feed_dict={rewards: total_reward_list})\n",
    "            writer.add_summary(summary, time_step)\n",
    "            # total_reward_list 는 초기화해준다.\n",
    "            total_reward_list = []\n",
    "        \n",
    "        # 에피소드 100번 중 한번 실행 \n",
    "        if episode % 100 == 0:\n",
    "            saver.save(sess, 'model/dqn.ckpt', global_step=time_step)   \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
